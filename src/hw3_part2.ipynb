{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Northeastern University</center></h1>\n",
    "<h1><center>EECE 7150 Autonomous Field Robotics</center></h1>\n",
    "<h1><center>HW3 Submission</center></h1>\n",
    "<h3><center>Yash Mewada</center></h3>\n",
    "<h3><center>Date: 25th Sept, 2023</center></h3>\n",
    "\n",
    "<h3>Part 1</h3>\n",
    "\n",
    "\n",
    "Modify your registration module from the previous home works to allow you to robustly match features across two images. Please continue to work with Python Notebooks as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import numpy as np\n",
    "from operator import sub\n",
    "import gtsam\n",
    "import gtsam.utils.plot as gtsam_plot\n",
    "import os\n",
    "import math\n",
    "\n",
    "\n",
    "class hw3():\n",
    "    def __init__(self,path_to_dataset):\n",
    "        \"\"\"\n",
    "        Constructor for hw3 class\n",
    "        Args:\n",
    "            path_to_dataset (str): path to dataset\n",
    "        \"\"\"\n",
    "        self.path_to_dataset = path_to_dataset\n",
    "    \n",
    "    def pixelCoorNormalise(self,coor,img)->tuple:\n",
    "        \"\"\"\n",
    "        Normalise pixel coordinate to range [-1,1]\n",
    "        Args:\n",
    "            coor (tuple): (x,y) coordinate\n",
    "            img (np.array): image\n",
    "\n",
    "        Returns:\n",
    "            tuple: (x,y) coordinate normalised\n",
    "        \"\"\"\n",
    "        x,y = coor\n",
    "        h,w = img.shape[:2]\n",
    "        return (2*x/w - 1,2*y/h - 1)\n",
    "\n",
    "    def pixelCoorDenormalise(self,coor,img)->tuple:\n",
    "        \"\"\"\n",
    "        Denormalise pixel coordinate to range [0,255]\n",
    "        Args:\n",
    "            coor (tuple): (x,y) coordinate\n",
    "            img (np.array): image\n",
    "\n",
    "        Returns:\n",
    "            tuple: (x,y) coordinate denormalised\n",
    "        \"\"\"\n",
    "        x,y = coor\n",
    "        h,w = img.shape[:2]\n",
    "        return (float((x+1)*w/2),float((y+1)*h/2))\n",
    "    \n",
    "    def performCLAHE(self,img):\n",
    "        \"\"\"\n",
    "        Perform CLAHE on image\n",
    "        Args:\n",
    "            img (np.array): image\n",
    "        \"\"\"\n",
    "        assert img is not None, \"file could not be read, check with os.path.exists()\"\n",
    "        # create a CLAHE object (Arguments are optional).\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        cl1 = clahe.apply(img)\n",
    "        return cl1\n",
    "        \n",
    "        \n",
    "    def siftDetctor(self,img1,img2):\n",
    "        \"\"\"\n",
    "        sift feature detector\n",
    "\n",
    "        Args:\n",
    "            img1 (np.array): image 1 \n",
    "            img2 (np.array): image 2\n",
    "\n",
    "        Returns:\n",
    "            img3 (np.array): image with matches\n",
    "            kp1 (list): Normalised list of keypoints in image 1\n",
    "            kp2 (list): Normalised list of keypoints in image 2\n",
    "        \"\"\"\n",
    "        \n",
    "        img1 = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n",
    "        img2 = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # img1 = self.performCLAHE(img1)\n",
    "        # img2 = self.performCLAHE(img2)\n",
    "        \n",
    "        sift = cv2.SIFT_create()\n",
    "        kp1,des1 = sift.detectAndCompute(img1,None)\n",
    "        kp2,des2 = sift.detectAndCompute(img2,None)\n",
    "    \n",
    "        # Brute Force Matching\n",
    "        matches = sorted(cv2.BFMatcher(cv2.NORM_L1,crossCheck=True).match(des1,des2),key=lambda x:x.distance)\n",
    "        goodkp2 = [kp2[mat.trainIdx] for mat in matches]\n",
    "        goodkp1 = [kp1[mat.queryIdx] for mat in matches]\n",
    "        #print(goodkp1[0].pt)\n",
    "\n",
    "        img3 = cv2.drawMatches(img1,kp1,img2,kp2,matches[:50],None,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        return img3,goodkp1,goodkp2\n",
    "    \n",
    "    def PoissonImageBlending(self,source, destination):\n",
    "        # create an all \"White\" mask: 255, if black mask is 0\n",
    "        mask = 255 * np.ones(destination.shape, destination.dtype) \n",
    "        # navigate the source img location\n",
    "        width, height, channels = source.shape\n",
    "        center = (height//2, width//2)\n",
    "\n",
    "        # using built-in funtion `cv2.seamlessClone` to acommpulish Poisson Image\n",
    "        blended = cv2.seamlessClone(destination, source, mask, center, 3) # cv::MIXED_CLONE = 2\n",
    "        output = blended\n",
    "        return output\n",
    "    \n",
    "    def featherBlending(self,A,B):\n",
    "        \"\"\"\n",
    "        Feather gradeint blending\n",
    "        \"\"\"\n",
    "        assert A is not None, \"file could not be read, check with os.path.exists()\"\n",
    "        assert B is not None, \"file could not be read, check with os.path.exists()\"\n",
    "        # generate Gaussian pyramid for A\n",
    "        G = A.copy()\n",
    "        gpA = [G]\n",
    "        for i in range(6):\n",
    "            G = cv2.pyrDown(G)\n",
    "            gpA.append(G)\n",
    "        # generate Gaussian pyramid for B\n",
    "        G = B.copy()\n",
    "        gpB = [G]\n",
    "        for i in range(6):\n",
    "            G = cv2.pyrDown(G)\n",
    "            gpB.append(G)\n",
    "        # generate Laplacian Pyramid for A\n",
    "        lpA = [gpA[5]]\n",
    "        for i in range(5,0,-1):\n",
    "            GE = cv2.pyrUp(gpA[i])\n",
    "            L = cv2.subtract(gpA[i-1],GE)\n",
    "            lpA.append(L)\n",
    "        # generate Laplacian Pyramid for B\n",
    "        lpB = [gpB[5]]\n",
    "        for i in range(5,0,-1):\n",
    "            GE = cv2.pyrUp(gpB[i])\n",
    "            L = cv2.subtract(gpB[i-1],GE)\n",
    "            lpB.append(L)\n",
    "        # Now add left and right halves of images in each level\n",
    "        LS = []\n",
    "        for la,lb in zip(lpA,lpB):\n",
    "            rows,cols,dpt = la.shape\n",
    "            ls = np.hstack((la[:,0:cols//2], lb[:,cols//2:]))\n",
    "            LS.append(ls)\n",
    "        # now reconstruct\n",
    "        ls_ = LS[0]\n",
    "        for i in range(1,6):\n",
    "            ls_ = cv2.pyrUp(ls_)\n",
    "            ls_ = cv2.add(ls_, LS[i])\n",
    "        # image with direct connecting each half\n",
    "        real = np.hstack((A[:,:cols//2],B[:,cols//2:]))\n",
    "        return ls_,real\n",
    "\n",
    "    def computeHomography(self,kp1,kp2,img1,img2):\n",
    "        \"\"\"\n",
    "        Compute homography matrix\n",
    "\n",
    "        Args:\n",
    "            kp1 (list): Normalised list of keypoints in image 1\n",
    "            kp2 (list): Normalised list of keypoints in image 2\n",
    "\n",
    "        Returns:\n",
    "            H (np.array): Homography matrix\n",
    "        \"\"\"\n",
    "        kp1 = np.array([kp.pt for kp in kp1])\n",
    "        kp2 = np.array([kp.pt for kp in kp2])\n",
    "        H,mask = cv2.findHomography(kp2,kp1,cv2.RANSAC)\n",
    "        # H,mask  = cv2.estimateAffinePartial2D(kp1,kp2)\n",
    "        # # H = H[:2,:]\n",
    "        # homogenous = np.array([[0,0,1]])\n",
    "        # H = np.concatenate((H,homogenous),axis=0)\n",
    "        # mathchesMask = mask.ravel().tolist()\n",
    "        return H\n",
    "\n",
    "    def computeAffine(self,kp1,kp2):\n",
    "        \"\"\"\n",
    "        Compute affine matrix\n",
    "        Args:\n",
    "            kp1 (_type_): _description_\n",
    "            kp2 (_type_): _description_\n",
    "            img1 (_type_): _description_\n",
    "            img2 (_type_): _description_\n",
    "        \"\"\"\n",
    "        kp1 = np.array([kp.pt for kp in kp1])\n",
    "        kp2 = np.array([kp.pt for kp in kp2])\n",
    "        H,_  = cv2.estimateAffinePartial2D(kp2,kp1)\n",
    "        H = H[:2,:]\n",
    "        homogenous = np.array([[0,0,1]])\n",
    "        H = np.concatenate((H,homogenous),axis=0)\n",
    "        \n",
    "        # scale = np.sqrt(H[0,0]**2 + H[1,0]**2)\n",
    "        # H[:2,:2] /= scale\n",
    "        affine_mat = H\n",
    "        return affine_mat\n",
    "    \n",
    "    def overlay_two_images(self,img1,img2,ignore_color = [0,0,0]):\n",
    "        ignore_color = np.asarray(ignore_color)\n",
    "        mask = (img2==ignore_color).all(-1,keepdims=True)\n",
    "        out = np.where(mask,img1,(img1 * 0.5 + img2 * 0.5).astype(img1.dtype))\n",
    "        return out\n",
    "    \n",
    "    def mean_blend(self,img1, img2):\n",
    "        assert(img1.shape == img2.shape)\n",
    "        locs1 = np.where(cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY) != 0)\n",
    "        blended1 = np.copy(img2)\n",
    "        blended1[locs1[0], locs1[1]] = img1[locs1[0], locs1[1]]\n",
    "        locs2 = np.where(cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY) != 0)\n",
    "        blended2 = np.copy(img1)\n",
    "        blended2[locs2[0], locs2[1]] = img2[locs2[0], locs2[1]]\n",
    "        blended = cv2.addWeighted(blended1, 0.8, blended2, 0.2, 0)\n",
    "        # blended = self.PoissonImageBlending(blended1,blended2)\n",
    "        return blended\n",
    "    \n",
    "    def warpImage(self,img1,img2,H,height,width,crop_black=True):\n",
    "        \"\"\"\n",
    "        Warp image 1 to image 2\n",
    "        \"\"\"\n",
    "        \n",
    "        h,w = img1.shape[:2]\n",
    "        pts = np.float32([[0,0],[0,h-1],[w-1,h-1],[w-1,0]]).reshape(-1,1,2)\n",
    "        dst = cv2.warpPerspective(img1,H,(width,height))\n",
    "        #print(img2.shape)\n",
    "        # blended = self.PoissonImageBlending(dst,img2)sss\n",
    "        \n",
    "        # blended = cv2.addWeighted(img2,0.5,dst,0.5,0)\n",
    "        blended = self.mean_blend(img2,dst)\n",
    "        final = np.zeros((img2.shape[0] + img2.shape[0],img2.shape[1] + img2.shape[1],3),np.uint8)\n",
    "        # # find center of the final canvas\n",
    "        \n",
    "        if crop_black:\n",
    "            final[0:img2.shape[0],0:img2.shape[1]] = blended\n",
    "            x,y,w,h = cv2.boundingRect(cv2.findNonZero(cv2.cvtColor(final, cv2.COLOR_BGR2GRAY)))\n",
    "            final = final[y:y+h,x:x+w]\n",
    "        \n",
    "        else:\n",
    "            (cx,cy) = (final.shape[1]//2,final.shape[0]//2)\n",
    "            # find center of the blended image\n",
    "            (bx,by) = (blended.shape[1]//2,blended.shape[0]//2)\n",
    "            # merge the blended image to the final canvas such that the center of the blended image is at the center of the final canvas\n",
    "            final[cy-by:cy-by+blended.shape[0],cx-bx:cx-bx+blended.shape[1]] = blended\n",
    "        # final = np.asarray(np.nonzero(final),dtype=np.uint8)\n",
    "        # #print(h,w)\n",
    "\n",
    "        return blended\n",
    "    \n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    GREEN = '\\033[32m'\n",
    "    BLUE = '\\033[34m'\n",
    "    YELLOW = '\\033[33m'\n",
    "    RESET = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findHallimgs(hw3:hw3,dataset:str,image29=False):\n",
    "    \n",
    "    image_lsit = []\n",
    "    # dataset = os.path.join(hw3.path_to_dataset,dataset)\n",
    "    # files = os.listdir(dataset)\n",
    "    # files.sort()\n",
    "    \n",
    "    if image29:\n",
    "        files = []\n",
    "        for root, dirs, f in os.walk(dataset):\n",
    "            for file in f:\n",
    "                # Get the full path of the file\n",
    "                file_path = os.path.abspath(os.path.join(root, file))\n",
    "                print(file_path)\n",
    "                files.append(file_path)\n",
    "    else:\n",
    "        dataset = os.path.join(hw3.path_to_dataset,dataset)\n",
    "        files = os.listdir(dataset)\n",
    "    files.sort()\n",
    "\n",
    "    for file,i in zip(files,range(len(files))):\n",
    "        if i < 2:\n",
    "            img = cv2.imread(os.path.join(dataset,file),cv2.IMREAD_GRAYSCALE)\n",
    "            if image29:\n",
    "                img = cv2.resize(img,(0,0),fx=1,fy=1)\n",
    "            clahe = cv2.createCLAHE(clipLimit=3, tileGridSize=(5,5))\n",
    "            cl1 = clahe.apply(img)\n",
    "            cl1 = cv2.cvtColor(cl1,cv2.COLOR_GRAY2BGR)\n",
    "            image_lsit.append(cl1)\n",
    "        else:\n",
    "            if image29:\n",
    "                img = cv2.resize(img,(0,0),fx=0.5,fy=0.5)\n",
    "            img = cv2.imread(os.path.join(dataset,file))\n",
    "            image_lsit.append(img)\n",
    "        \n",
    "    H_d = {}\n",
    "    n_features = {}\n",
    "    aff1 = {}\n",
    "    for i in range(len(image_lsit)-1):\n",
    "        key = str(i)+str(i+1)\n",
    "        # print(colors.BLUE+\"Computing Homography for image pair: \"+key+colors.RESET)\n",
    "        _,kp1,kp2 = hw3.siftDetctor(image_lsit[i],image_lsit[i+1])\n",
    "        n_features[i] = len(kp1)\n",
    "        H = hw3.computeHomography(kp1,kp2,image_lsit[i],image_lsit[i+1])\n",
    "        aff1[key] = hw3.computeAffine(kp1,kp2)\n",
    "        H_d[key] = H\n",
    "    # print(n_features.keys())\n",
    "    print(\"Consecutiive frames H:\",H_d.keys()) \n",
    "    used_keys = []\n",
    "    H_new = {}\n",
    "\n",
    "                    \n",
    "    aff = {}\n",
    "    H_init = H_d['01']\n",
    "    i = 0\n",
    "    for key1 in H_d:\n",
    "        for key2 in H_d:\n",
    "            new_key = key1[0] + key2[1]  # Generate the new key\n",
    "            swapped_key = key2[0] + key1[1]  # Generate the swapped key\n",
    "            # Check conditions to ensure that swapped keys and duplicates are avoided\n",
    "            if (key1 != key2) and (new_key not in used_keys) and (swapped_key not in used_keys):\n",
    "                # print(\"Non consecutive frames\")\n",
    "                cumulative_H = H_init\n",
    "                current_key = key1\n",
    "                while current_key[1] != key2[1]:\n",
    "                    next_key = f\"{current_key[1]}{str(int(current_key[1]) + 1)}\"\n",
    "                    cumulative_H = np.dot(cumulative_H,H_d[next_key])  # Adjust this based on your key format\n",
    "                    cumulative_H[2,2] = 1\n",
    "                    current_key = next_key\n",
    "                #     print(\"inside\",i)\n",
    "                # print(\"out\")\n",
    "                _,kp1,kp2 = hw3.siftDetctor(image_lsit[int(key1[0])],image_lsit[int(key2[1])])\n",
    "                aff[new_key] = hw3.computeAffine(kp1,kp2)\n",
    "                # H_new[new_key] = hw3.computeHomography(kp1,kp2,image_lsit[int(key1[0])],image_lsit[int(key2[1])])\n",
    "                # print(f\"{colors.YELLOW}Affine matrix for image pair: {key1[0]}\"  \"{key2[1]} computed{colors.RESET}\")\n",
    "                H_new[new_key] = cumulative_H\n",
    "                used_keys.append(new_key)\n",
    "                i += 1\n",
    "                \n",
    "    aff.update(aff1)\n",
    "    H_d.update(H_new)\n",
    "    \n",
    "    h00 = np.array([[1,0,0],[0,1,0],[0,0,1]],dtype=np.float32)\n",
    "    my_h = {'00':h00}\n",
    "    my_a = {'00':h00}\n",
    "    for keys,values in H_d.items():\n",
    "        if keys[0] == '0':\n",
    "            my_h[keys] = values\n",
    "    \n",
    "    for keys,values in aff.items():\n",
    "        if keys[0] == '0':\n",
    "            my_a[keys] = values\n",
    "    # print(f\"{colors.GREEN}Homography matrix for all image pairs computed{colors.RESET}\",H.keys())\n",
    "    print(f\"{colors.GREEN}Homography matrix for all image pairs computed{colors.RESET}\",H_d.keys())\n",
    "    print(f\"{colors.GREEN}Affine matrix for all image pairs computed{colors.RESET}\",aff.keys())\n",
    "    print(f\"{colors.GREEN}Only H{colors.RESET}\",my_h.keys())\n",
    "\n",
    "    return H_d,image_lsit,my_h,n_features,aff\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addBorder(img, rect):\n",
    "    top, bottom, left, right = int(0), int(0), int(0), int(0)\n",
    "    x, y, w, h = rect\n",
    "    tl = (x, y)    \n",
    "    br = (x + w, y + h)\n",
    "    if tl[1] < 0:\n",
    "        top = -tl[1]\n",
    "    if br[1] > img.shape[0]:\n",
    "        bottom = br[1] - img.shape[0]\n",
    "    if tl[0] < 0:\n",
    "        left = -tl[0]\n",
    "    if br[0] > img.shape[1]:\n",
    "        right = br[0] - img.shape[1]\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right,\n",
    "                            cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "    orig = (left, top)\n",
    "    return img, orig\n",
    "\n",
    "def patchPano(hw3:hw3,img1, img2, orig1=(0,0), orig2=(0,0)):\n",
    "    # bottom right points\n",
    "    br1 = (img1.shape[1] - 1, img1.shape[0] - 1)\n",
    "    br2 = (img2.shape[1] - 1, img2.shape[0] - 1)\n",
    "    # distance from orig to br\n",
    "    diag2 = tuple(map(sub, br2, orig2))\n",
    "    # possible pano corner coordinates based on img1\n",
    "    extremum = np.array([(0, 0), br1,\n",
    "                tuple(map(sum, zip(orig1, diag2))),\n",
    "                tuple(map(sub, orig1, orig2))])\n",
    "    bb = cv2.boundingRect(extremum)\n",
    "    # patch img1 to img2\n",
    "    pano, shift = addBorder(img1, bb)\n",
    "    orig = tuple(map(sum, zip(orig1, shift)))\n",
    "    idx = np.s_[orig[1] : orig[1] + img2.shape[0] - orig2[1],\n",
    "                orig[0] : orig[0] + img2.shape[1] - orig2[0]]\n",
    "    subImg = img2[orig2[1] : img2.shape[0], orig2[0] : img2.shape[1]]\n",
    "    pano[idx] = hw3.mean_blend(pano[idx], subImg)\n",
    "    # pano[idx] = hw3.featherBlending(pano[idx], subImg)\n",
    "    return np.array(pano), orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimFind(img,H):\n",
    "    transPts=[]\n",
    "    img1=img\n",
    "    H1=H\n",
    "    h1,w1 = img1.shape[:2] #grab height and width from input image\n",
    "    pts1 = ((0,0),(w1,0),(w1,h1),(0,h1)) #create an array of just the corners in order bottom left, top left, top right, bottom right\n",
    "\n",
    "    for pt in pts1:\n",
    "        transPts.append(np.matmul(H1,np.array([pt[0],pt[1],1]))) #matmul the corners by the homography\n",
    "\n",
    "    #this little x_min/max section was inspired by the great Xavier Hubbard \n",
    "    x_min = min(pt[0]/pt[2] for pt in transPts)\n",
    "    x_max = max(pt[0]/pt[2] for pt in transPts)\n",
    "    #doing the same for y, I think it makes the pano look worse though...\n",
    "    y_min = min(pt[1]/pt[2] for pt in transPts)\n",
    "    y_max = max(pt[1]/pt[2] for pt in transPts)\n",
    "    \n",
    "\n",
    "    if x_min<0:\n",
    "        x_shift = abs(x_min)\n",
    "    else:\n",
    "        x_shift = 0\n",
    "\n",
    "    if y_min<0:\n",
    "        y_shift = abs(y_min)\n",
    "    else:\n",
    "        y_shift = 0\n",
    "\n",
    "    transH = np.array([[1,0,np.ceil(x_shift)],[0,1,0],[0,0,1]])\n",
    "\n",
    "    dim = (int(x_max+x_shift),int(y_max+y_shift))\n",
    "    return dim, transH\n",
    "\n",
    "def cavas_dim(image_list,H):\n",
    "    min_x = float('inf')\n",
    "    max_x = -float('inf')\n",
    "    min_y = float('inf')\n",
    "    max_y = -float('inf')\n",
    "    \n",
    "    for i,value in zip(range(len(image_list)),H.values()):\n",
    "        h,w,_ = image_list[i].shape\n",
    "        corners = np.float32([[0,0],[w,0],[w,h],[0,h]])\n",
    "        transformed_corners = cv2.perspectiveTransform(corners.reshape(-1,1,2),value).reshape(-1,2)\n",
    "        corners_tf = np.vstack((transformed_corners,corners.reshape(-1,2)))\n",
    "        \n",
    "        min_x_i = int(np.min(corners_tf[:, 0]))\n",
    "        max_x_i = int(np.max(corners_tf[:, 0]))\n",
    "        min_y_i = int(np.min(corners_tf[:, 1]))\n",
    "        max_y_i = int(np.max(corners_tf[:, 1]))\n",
    "\n",
    "        # Update the overall minimum and maximum coordinates\n",
    "        min_x = min(min_x, min_x_i)\n",
    "        max_x = max(max_x, max_x_i)\n",
    "        min_y = min(min_y, min_y_i)\n",
    "        max_y = max(max_y, max_y_i)\n",
    "    \n",
    "    return min_x,max_x,min_y,max_y\n",
    "    \n",
    "\n",
    "def getShift(image,H):\n",
    "    corners = np.float32([[0,0],[0,image.shape[0]-1],[image.shape[1]-1,image.shape[0]-1],[image.shape[1]-1,0]])\n",
    "    H_corners = []\n",
    "    # transform these point to the respective homography\n",
    "    for p in corners:\n",
    "        # #print(p)\n",
    "        p = np.array([p[0],p[1],1])\n",
    "        p = np.dot(H,p)\n",
    "        p = p/p[2]\n",
    "        H_corners.append(p)\n",
    "    \n",
    "    shift = (float(\"inf\"),float(\"inf\"))\n",
    "    shift_x = 0\n",
    "    shift_y = 0\n",
    "    for p in H_corners:\n",
    "        # get the least negative x and y\n",
    "        if p[0] < shift[0]:\n",
    "            shift_x = int(p[0])\n",
    "        if p[1] < shift[1]:\n",
    "            shift_y = int(p[1])\n",
    "        else:\n",
    "            shift_x = 0\n",
    "            shift_y = 0\n",
    "        shift = (shift_x,shift_y)\n",
    "\n",
    "    return shift    \n",
    "    \n",
    "def stitchformHdynamic(hw3:hw3,H,image_list):\n",
    "    # stich the images based on the dictionary of homography matrices\n",
    "    \n",
    "    min_x,max_x,min_y,max_y = cavas_dim(image_list,H)\n",
    "    # odom = []\n",
    "    print(\"min_x,max_x,min_y,max_y\",min_x,max_x,min_y,max_y)\n",
    "    canvas_width = max_x - min_x\n",
    "    canvas_height = max_y - min_y\n",
    "    print(\"canvas_width,canvas_height\",canvas_width,canvas_height)\n",
    "    \n",
    "    if canvas_height>10e4 or canvas_width>10e4:\n",
    "        return np.zeros((100, 100, 3), np.uint8)\n",
    "    \n",
    "    canvas = np.zeros((canvas_height,canvas_width,3),np.uint8)\n",
    "    tx = -min_x\n",
    "    ty = -min_y\n",
    "    t_mat = np.array([[1,0,tx],[0,1,ty],[0,0,1]],dtype=np.float32)\n",
    "    \n",
    "    print(\"H length\",len(H))\n",
    "    for i,value in zip(range(len(image_list)),H.values()):\n",
    "        # print(\"i\",i)\n",
    "        # if i == 0:\n",
    "        #     canvas = cv2.warpPerspective(image_list[0],t_mat,(canvas_width,canvas_height))\n",
    "        # else:\n",
    "        H_new = t_mat@value\n",
    "        warped_image = cv2.warpPerspective(image_list[i],H_new,(canvas_width,canvas_height))\n",
    "        \n",
    "        canvas = hw3.mean_blend(canvas,warped_image)\n",
    "        # H_new = t_mat@value\n",
    "        # warped_image = cv2.warpPerspective(image_list[i],H_new,(canvas_width,canvas_height))\n",
    "        \n",
    "        # canvas = hw3.mean_blend(canvas,warped_image)\n",
    "    \n",
    "    x,y,w,h = cv2.boundingRect(cv2.findNonZero(cv2.cvtColor(canvas, cv2.COLOR_BGR2GRAY)))\n",
    "    return canvas[y:y+h,x:x+w],t_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Detector** - In part 1, I found that Clahe only worked for me for inital 2/3 images, hence I only applied CLAHE to those images.This code uses Brute Force Matcher (BF Matcher) to find the matches and get the good matches out of those. I am using SIFT detector here. I also tested it with ORB, but the output by ORB gets worse after 4 images, (which should not be the case as in certain papers they claim that ORB outperforms SIFT because of its bitwise descriptor).\n",
    "\n",
    "**RANSAC** - The Homography matrix estimation is done by RANSAC. I also tried estimating the affine transformation matrix,but the results were a bit off.\n",
    "\n",
    "**Image Stitching and Panorama generation** - For panorama generation and estimating the location of image before sticching, I take four corners of the image and then multiply it with its respective H matrix. Then in those four warped points if any of the *x,y* is out of canvas i.e negative, I shit the previous panorama/image by the absolute value of that shift i.e (np.roll), hence giving me shift in X and Y axes. After the shifts are available I warp and stitch the images.\n",
    "\n",
    "**Mean/Alpha Bleding** - I found while naively applying alpha blending is it also blends the black pixels of the warped image hence making the blended image darker and darker as the number of images increases, hence I applied what I call a \"*Mean Blending*\", here I copy the non zeros pixels of both the images and then perform Alpha blending hence, only the non zero pixels gets blended.\n",
    "\n",
    "**Note** - The implementation of all above methods is in *main* function below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part 2</h3>\n",
    "Go around the entire loop of 6 images and set up the problem by outputting vertices and edges in terms of a factor graph\n",
    "\n",
    "**Pose 2 genetation** - We all know that the H matrix is a combination of camera intrinsics, Rotation and Translation matrix as.\n",
    "$\\mathbf{H} = \\mathbf{K} \\cdot \\mathbf{R} \\cdot \\mathbf{T}$, Where:\n",
    "\\begin{align*}\n",
    "    \\mathbf{K} & : \\text{Intrinsic camera matrix} \\\\\n",
    "    \\mathbf{R} & : \\text{Extrinsic rotation matrix} \\\\\n",
    "    \\mathbf{T} & : \\text{Extrinsic translation vector} \\\\\n",
    "\\end{align*}\n",
    "Hence we can obtain the tranlation from the right top most element of the H matrix as:\n",
    "$\\mathbf{H} =\n",
    "\\begin{bmatrix}\n",
    "    h_{11} & h_{12} & x \\\\\n",
    "    h_{21} & h_{22} & y \\\\\n",
    "    h_{31} & h_{32} & 1 \\\\\n",
    "\\end{bmatrix}$\n",
    "Then I did SVD of the top left matrix of this H matrix to obtaint the Rotation matrix and then $\\tan^{-1}$ will give me the pose of the image. Although there was no difference in this case where we directly take the top left corner of the H matrix to compute the value of $\\theta$ or decompose it and then take proper rotation matrix because ther vertices are just points, but decomposing the top left of H matrix to obtain R matrix made sense theoretically.\n",
    "\n",
    "This pose is then further given to the factor graph to generate the graph and then eventually optimise it using *Levenberg-Marquardt* algorithm.\n",
    "**Note** - Plot graph function is taken from ZZ's code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pose2(hw3:hw3,dataset,image29=False,t_mat=None):\n",
    "    H_dict,image_list,_,n,_ = findHallimgs(hw3,dataset,image29)\n",
    "    MAX_MATCHES = max(n.values())\n",
    "    # #print(\"H\",H)\n",
    "    w,h = image_list[0].shape[1],image_list[0].shape[0]\n",
    "        \n",
    "    #prior mean and noise model and taking center of the image as the prior pose\n",
    "    priorMean = gtsam.Pose2(w/2,h/2,0)\n",
    "    PRIOR_NOISE_MODEL = gtsam.noiseModel.Diagonal.Sigmas(np.array([0.3, 0.3, 0.1]))\n",
    "    ODOM_NOISE_MODEL = gtsam.noiseModel.Diagonal.Sigmas(np.array([0.2, 0.2, 0.1]))\n",
    "    \n",
    "    graph = gtsam.NonlinearFactorGraph()\n",
    "    graph.add(gtsam.PriorFactorPose2(0, priorMean, PRIOR_NOISE_MODEL))\n",
    "    \n",
    "    initial = gtsam.Values()\n",
    "    initial.insert(0, gtsam.Pose2())\n",
    "    i = 1\n",
    "    \n",
    "    for key, value in H_dict.items():\n",
    "        if key.startswith('0'):\n",
    "            H = value\n",
    "            x,y = H[0,-1],H[1,-1]\n",
    "            U, _, Vt = np.linalg.svd(H[:2, :2])\n",
    "            R = np.dot(U, Vt)\n",
    "            theta = np.arctan2(H[1,0],H[0,0])\n",
    "            pose = gtsam.Pose2(x,y,theta)\n",
    "            initial.insert(i, pose)\n",
    "            i += 1\n",
    "    \n",
    "    def getNoise(x,y,th,n):\n",
    "        # A part of this function is implemented form Curtis's Code\n",
    "        ERF_DIV = 50\n",
    "        LIN_DIV = 50\n",
    "        covar_multiplier = math.exp(n - MAX_MATCHES)\n",
    "        exx = abs(x / LIN_DIV) * covar_multiplier\n",
    "        eyy = abs(y / LIN_DIV) * covar_multiplier\n",
    "        ett = abs(th / (LIN_DIV * 10.0)) * covar_multiplier\n",
    "        # covar_multiplier = 1/math.erf((n-3)/ERF_DIV)\n",
    "\n",
    "        # exx = abs(x/LIN_DIV) * covar_multiplier\n",
    "        # eyy = abs(y/LIN_DIV) * covar_multiplier\n",
    "        # ett = abs(th/(LIN_DIV * 10.0)) * covar_multiplier\n",
    "        \n",
    "        diag_noise = np.array([exx, eyy, ett])\n",
    "        return gtsam.noiseModel.Diagonal.Sigmas(gtsam.Point3(diag_noise))\n",
    "    \n",
    "    i = 0\n",
    "    for key,value in H_dict.items():\n",
    "        H = value\n",
    "        x,y = H[0,-1],H[1,-1]\n",
    "        U, _, Vt = np.linalg.svd(H[:2, :2])\n",
    "        R = np.dot(U, Vt)\n",
    "        theta = np.arctan2(R[1,0],R[0,0])\n",
    "        src = int(key[0])\n",
    "        dst = int(key[1])\n",
    "        if i < 5:\n",
    "            ODOM_NOISE_MODEL = getNoise(x,y,theta,n[i])\n",
    "        else:\n",
    "            ODOM_NOISE_MODEL = gtsam.noiseModel.Diagonal.Sigmas(np.array([0.2, 0.2, 0.1]))    \n",
    "        graph.add(gtsam.BetweenFactorPose2(src,dst,gtsam.Pose2(x,y,theta),ODOM_NOISE_MODEL))\n",
    "        i += 1\n",
    "\n",
    "    #print(\"\\nFactor Graph:\\n{}\".format(graph))\n",
    "    return graph,initial\n",
    "\n",
    "def plot_graph(values,graph,marginals,res):\n",
    "    #print(\"Values:\\n{}\".format(values))\n",
    "    #print(\"Marginals:\\n{}\".format(marginals))\n",
    "    for vertex_index in range(values.size()):\n",
    "        gtsam_plot.plot_pose2(0, values.atPose2(\n",
    "            vertex_index), 0.5, marginals.marginalCovariance(vertex_index))\n",
    "\n",
    "    for edge_index in range(graph.size())[1:]:\n",
    "        key1, key2 = graph.at(edge_index).keys()\n",
    "\n",
    "        start_pose = values.atPose2(key1)\n",
    "        end_pose = values.atPose2(key2)\n",
    "        plt.plot([start_pose.x(), end_pose.x()],\n",
    "                 [start_pose.y(), end_pose.y()],\n",
    "                 color='blue')\n",
    "    plt.title(res)\n",
    "    plt.grid()\n",
    "    plt.savefig(res+\".png\")\n",
    "    plt.show()\n",
    "\n",
    "def reconstructH(result,image_list,prior,t_mat):\n",
    "    poses = gtsam.utilities.allPose2s(result)\n",
    "    H_dict = {}\n",
    "    w,h = image_list[0].shape[1],image_list[0].shape[0]\n",
    "    # print(poses.size())affine_H1\n",
    "    for i in range(poses.size()):\n",
    "        key = str(i)+str(i+1)\n",
    "        pose = poses.atPose2(i)\n",
    "        affine_H = t_mat@pose.matrix()\n",
    "        # if i == 0:\n",
    "        #     affine_H = t_mat@affine_H\n",
    "        # if prior == False:\n",
    "        #     trans = np.eye(3)\n",
    "        #     trans[:2,2] -= np.array([w/2, h/2]).T\n",
    "        #     affine_tf = affine_H@trans\n",
    "        #     # if i > 0:\n",
    "        #     H_dict[key] = affine_tf\n",
    "        # else:\n",
    "        H_dict[key] = affine_H\n",
    "        # H_dict[key] = affine_H\n",
    "    print(\"reconstructed keys\",H_dict.keys())\n",
    "    return H_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way I have calculated the covariance is based on the number of matches between the images, the more the matches the less the covariance. I have used the following formula to calculate the covariance:\n",
    "\\begin{align*}\n",
    "    \\text{covar\\_multiplier} = \\exp(n - \\text{MAX\\_MATCHES})\\\\\n",
    "    exx = \\left| \\frac{x}{30} \\right| \\cdot \\text{covar\\_multiplier}\\\\\n",
    "    eyy = \\left| \\frac{y}{30} \\right| \\cdot \\text{covar\\_multiplier}\\\\\n",
    "    ett = \\left| \\frac{\\theta}{30 \\cdot 10.0} \\right| \\cdot \\text{covar\\_multiplier}\\\\\n",
    "    \\text{diag\\_noise} = [ \\text{exx}, \\text{eyy}, \\text{ett} ]\n",
    "\\end{align*}\n",
    "The above expression is called exponential scaling of covariance based on the number of matches. Exponential scaling, which can provide more drastic adjustments based on the number of matches.\n",
    "\n",
    "\n",
    "LIN_DIV is a constant or parameter that is used to divide the displacement values dx, dy, and dth in order to normalize them before applying the scaling factor covar_multiplier. It's essentially a divisor used to adjust the sensitivity of the covariance scaling based on the size of the displacement.\n",
    "\n",
    "The choice of LIN_DIV should be based on the scale and units of your motion or displacement measurements. It can be used to control how much the covariance scales with respect to the magnitude of motion. A smaller LIN_DIV will result in larger covariances for the same magnitude of motion, while a larger LIN_DIV will result in smaller covariances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_6():\n",
    "    print(f\"{colors.BLUE}OUTPUT for 6 images{colors.RESET}\")\n",
    "    dataset = \"/home/mewada/Documents/AFR/eece7150/HW3/Dataset/6Images/\"\n",
    "    hw31 = hw3(dataset)\n",
    "    H_d,image_list,H,_,_ = findHallimgs(hw31,dataset,image29=False)\n",
    "    \n",
    "    pano,t_mat = stitchformHdynamic(hw31,H,image_list)\n",
    "    cv2.imshow(\"pano\",pano)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    print(f\"{colors.BLUE}Before Optimisation{colors.RESET}\")\n",
    "    graph,initial = create_pose2(hw31,dataset,image29=False,t_mat=t_mat)\n",
    "    print(f\"{colors.BLUE}Initials{colors.RESET}\",initial)\n",
    "    marginals = gtsam.Marginals(graph,initial)\n",
    "    res = \"Before Optimisation\"\n",
    "    plot_graph(initial,graph,marginals,res)\n",
    "    initial_h = reconstructH(initial,image_list,prior=True,t_mat=t_mat)\n",
    "    pano_before_opt,_ = stitchformHdynamic(hw31,initial_h,image_list)\n",
    "    \n",
    "    cv2.imshow(\"pano_before_opt\",pano_before_opt)\n",
    "    # plt.imshow(pano_before_opt)\n",
    "    # plt.title(res)\n",
    "    # plt.axis('equal')\n",
    "    # plt.savefig(\"pano_before_opt.png\")\n",
    "    # plt.show()\n",
    "    # plt.close()\n",
    "    \n",
    "    print(f\"{colors.GREEN}After Optimisation{colors.RESET}\")\n",
    "    params = gtsam.LevenbergMarquardtParams()\n",
    "    optimizer = gtsam.LevenbergMarquardtOptimizer(graph, initial, params)\n",
    "    result = optimizer.optimize()\n",
    "    res = \"After Optimisation\"\n",
    "    print(f\"{colors.BLUE}Result{colors.RESET}\",result)\n",
    "    marginals = gtsam.Marginals(graph, result)\n",
    "    plot_graph(result, graph, marginals,res)\n",
    "    result_h = reconstructH(result,image_list,prior=True,t_mat=t_mat)\n",
    "    \n",
    "    pano_after_opt,_ = stitchformHdynamic(hw31,result_h,image_list)\n",
    "    cv2.imshow(\"pano_after_opt\",pano_after_opt)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    # plt.imshow(pano_after_opt)\n",
    "    # plt.title(res)\n",
    "    # plt.axis('equal')\n",
    "    # plt.savefig(\"pano_after_opt.png\")\n",
    "    # plt.show()\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_29():\n",
    "    print(f\"{colors.BLUE}OUTPUT for 6 images{colors.RESET}\")\n",
    "    dataset = \"/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/\"\n",
    "    hw31 = hw3(dataset)\n",
    "    H_d,image_list,H,_,_ = findHallimgs(hw31,dataset,image29=True)\n",
    "    \n",
    "    pano,t_mat = stitchformHdynamic(hw31,H,image_list)\n",
    "    cv2.imshow(\"pano\",pano)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    print(f\"{colors.BLUE}Before Optimisation{colors.RESET}\")\n",
    "    graph,initial = create_pose2(hw31,dataset,image29=True,t_mat=t_mat)\n",
    "    print(f\"{colors.BLUE}Initials{colors.RESET}\",initial)\n",
    "    marginals = gtsam.Marginals(graph,initial)\n",
    "    res = \"Before Optimisation\"\n",
    "    plot_graph(initial,graph,marginals,res)\n",
    "    initial_h = reconstructH(initial,image_list,prior=True,t_mat=t_mat)\n",
    "    pano_before_opt,_ = stitchformHdynamic(hw31,initial_h,image_list)\n",
    "    \n",
    "    cv2.imshow(\"pano_before_opt\",pano_before_opt)\n",
    "    # plt.imshow(pano_before_opt)\n",
    "    # plt.title(res)\n",
    "    # plt.axis('equal')\n",
    "    # plt.savefig(\"pano_before_opt.png\")\n",
    "    # plt.show()\n",
    "    # plt.close()\n",
    "    \n",
    "    print(f\"{colors.GREEN}After Optimisation{colors.RESET}\")\n",
    "    params = gtsam.LevenbergMarquardtParams()\n",
    "    optimizer = gtsam.LevenbergMarquardtOptimizer(graph, initial, params)\n",
    "    result = optimizer.optimize()\n",
    "    res = \"After Optimisation\"\n",
    "    print(f\"{colors.BLUE}Result{colors.RESET}\",result)\n",
    "    marginals = gtsam.Marginals(graph, result)\n",
    "    plot_graph(result, graph, marginals,res)\n",
    "    result_h = reconstructH(result,image_list,prior=True,t_mat=t_mat)\n",
    "    \n",
    "    pano_after_opt,_ = stitchformHdynamic(hw31,result_h,image_list)\n",
    "    cv2.imshow(\"pano_after_opt\",pano_after_opt)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    # plt.imshow(pano_after_opt)\n",
    "    # plt.title(res)\n",
    "    # plt.axis('equal')\n",
    "    # plt.savefig(\"pano_after_opt.png\")\n",
    "    # plt.show()\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mOUTPUT for 6 images\u001b[0m\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/fourth column/4.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/fourth column/6.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/fourth column/2.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/fourth column/3.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/fourth column/7.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/fourth column/1.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/fourth column/5.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/fourth column/8.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/second column/ESC.970622_025500.0621.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/second column/ESC.970622_025447.0620.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/second column/ESC.970622_025526.0623.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/second column/ESC.970622_025513.0622.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/second column/ESC.970622_025420.0618.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/second column/ESC.970622_025434.0619.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/first column/4.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/first column/6.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/first column/2.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/first column/3.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/first column/7.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/first column/1.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/first column/5.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/third column/ESC.970622_030219.0654.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/third column/ESC.970622_030232.0655.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/third column/ESC.970622_030245.0656.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/third column/ESC.970622_030140.0651.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/third column/ESC.970622_030258.0657.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/third column/ESC.970622_030153.0652.tif\n",
      "/home/mewada/Documents/AFR/eece7150/HW3/Dataset/29images/29images/third column/ESC.970622_030206.0653.tif\n",
      "Consecutiive frames H: dict_keys(['01', '12', '23', '34', '45', '56', '67', '78', '89', '910', '1011', '1112', '1213', '1314', '1415', '1516', '1617', '1718', '1819', '1920', '2021', '2122', '2223', '2324', '2425', '2526', '2627'])\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    images_6()\n",
    "    # images_29()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is seen from the above output that the factor graph optimization worked but not significantly in 6 images case as the initial estimates were good but there is a slight decrease in errors as the graph optimises.\n",
    "I tried it with 29 images, but the kernel was crashing as I was trying to process all the images at once. If time persists I will try and upload the output of all 29 images, but for now I tried just the third column of the 29 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the factor graph optimization is done, but the plotting of images after was a bit difficult as I wanted to experiment with a dynamically chaning canvas size rather than any constant valued.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
